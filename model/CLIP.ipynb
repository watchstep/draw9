{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLp-fA9Y31iL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "import os\n",
        "import glob as gb\n",
        "import pathlib\n",
        "import random\n",
        "import torch\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# https://github.com/openai/CLIP/blob/fcab8b6eb92af684e7ff0a904464be7b99b49b88/notebooks/Prompt_Engineering_for_ImageNet.ipynb\n",
        "# https://github.com/openai/CLIP/issues/164\n",
        "# https://github.com/openai/CLIP/issues/83"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_jYyAsY3fly"
      },
      "outputs": [],
      "source": [
        "# Setting seed\n",
        "def set_seed(seed):\n",
        "    # os.environ['PYTHONASHSEED'] = 0 무작위화 비활성화\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!\n",
        "pip install wandb\n",
        "wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8X2AHzz6els"
      },
      "outputs": [],
      "source": [
        "set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gnn1VHU33zT",
        "outputId": "80b4296c-6347-4276-de30-f00582685435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/airplane.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bicycle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bird.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bus.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cup.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/duck.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fish.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flower.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hamburger.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/house.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ice%20cream.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/light%20bulb.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lion.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/nose.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/monkey.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/moon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pencil.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pig.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rabbit.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shoe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spider.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sun.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tree.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/umbrella.npy\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Get image labels\n",
        "def get_labels(labels_path='/content/30_labels.txt'):\n",
        "  f = open(labels_path, 'r')\n",
        "  labels = f.readlines()\n",
        "  f.close()\n",
        "  \n",
        "  labels = [l.replace('\\n', '').replace(' ', '_') for l in labels]\n",
        "  return labels\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def load_dataset(dataset_path='./dataset/'):\n",
        "  # make directory\n",
        "  try:\n",
        "    if not os.path.exists(dataset_path):\n",
        "      os.makedirs(dataset_path)\n",
        "  except:\n",
        "      None\n",
        "      \n",
        "  # get data from web\n",
        "  labels = get_labels()\n",
        "  base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "  for label in labels:\n",
        "    label_url = label.replace('_', '%20')\n",
        "    npy_url = base_url + label_url + '.npy'\n",
        "    print(npy_url)\n",
        "    urllib.request.urlretrieve(npy_url, dataset_path + label + '.npy')\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUVdo2xm5Q_9",
        "outputId": "1b5ba694-2cb5-43cb-c05b-8a06ce7cb69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 71.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 80.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb3vJP_F4gSa"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training\n",
        "def check_device():\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = torch.device('cuda')\n",
        "    else:\n",
        "        DEVICE = torch.deivce('cpu')\n",
        "    \n",
        "    print('Using Pytorch version : ',  torch.__version__, 'DEVICE : ', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA7rgbox5hpT",
        "outputId": "db68bb83-2c3f-443c-f977-6fa64952c180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Pytorch version :  1.12.1+cu113 DEVICE :  cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = check_device()\n",
        "\n",
        "# model = model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3eH2uyc82n4",
        "outputId": "9f6e2992-3adc-47ef-95a7-e6f2eaa6e270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-b5i3zuzh\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-b5i3zuzh\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=4739ac0ec7288351eb694ccad90345acc5ec206e42d56fbc27f77d1f2df1be87\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n9zcbdx1/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HoJmvYJ8-NP",
        "outputId": "7952c612-9cfe-4971-827c-71e006e1b883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaqR2Tb9DOI8",
        "outputId": "240553ce-8c40-4c58-d820-46dc039fc343"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 244M/244M [00:04<00:00, 58.8MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 102,007,137\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"RN50\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtNUus8zDS8q",
        "outputId": "f723e54c-0c44-4bd6-ec45-ef68d0971ab8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x7faa28ab9e50>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYEjMs9qExep"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "import numpy as np\n",
        "import glob as gb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, random_split, TensorDataset\n",
        "import os\n",
        "\n",
        "\n",
        "class QuickDrawDataset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "def prepare_img_dataset(npy_files_path='./dataset/*.npy',test_ratio=0.2, max_items_per_class=10000):\n",
        "    npy_files = gb.glob(npy_files_path)\n",
        "\n",
        "    #initialize variables \n",
        "    X = np.empty([0, 784]) # 28*28 =784\n",
        "    y = np.empty([0])\n",
        "    classes = []\n",
        "\n",
        "    #load a subset of the data to memory \n",
        "    for idx, npy_file in enumerate(npy_files):\n",
        "        data = np.load(npy_file)\n",
        "        data = data[0:max_items_per_class, :]\n",
        "        labels = np.full(data.shape[0], idx)\n",
        "\n",
        "        X = np.concatenate((X, data), axis=0)\n",
        "        y = np.append(y, labels)\n",
        "    \n",
        "        label, extension = os.path.splitext(os.path.basename(npy_file))\n",
        "        classes.append(label)\n",
        "\n",
        "    data = None\n",
        "    labels = None\n",
        "    \n",
        "    # transform to torch tensor\n",
        "    X_tensor = torch.Tensor(X)\n",
        "    X_tensor = X_tensor.reshape(X_tensor.shape[0], 1, 28, 28)\n",
        "    # normalizatoin\n",
        "    X_tensor /= 255.0\n",
        "    y_tensor = torch.Tensor(y)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=test_ratio, random_state=1, stratify=y_tensor)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1, stratify=y_test)\n",
        "    \n",
        "    # create dataset\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    \n",
        "    # caculate mean & std\n",
        "    means = torch.zeros(1)\n",
        "    stds = torch.zeros(1)\n",
        "    \n",
        "    for img, label in train_dataset:\n",
        "        means += torch.mean(img, dim = (1,2))\n",
        "        stds += torch.std(img, dim = (1,2))\n",
        "        \n",
        "    means /= len(train_dataset)\n",
        "    stds /= len(train_dataset)\n",
        "    \n",
        "    print(f'Means: {means}')\n",
        "    print(f'STDs: {stds}')\n",
        "    \n",
        "    # transform\n",
        "    transforms_train = transforms.Compose([\n",
        "        transforms.Normalize((0.1702,), (0.3224)),\n",
        "        transforms.ToPILImage()\n",
        "    ])\n",
        "    \n",
        "    transforms_test = transforms.Compose([\n",
        "        transforms.Normalize((0.1702,), (0.3224)),\n",
        "        transforms.ToPILImage()\n",
        "    ])\n",
        "    \n",
        "    train_dataset = QuickDrawDataset(subset=train_dataset, transform=transforms_train)\n",
        "    val_dataset = QuickDrawDataset(subset=val_dataset, transform=transforms_test)\n",
        "    test_dataset = QuickDrawDataset(subset=test_dataset, transform=transforms_test)\n",
        "    \n",
        "    \n",
        "    return dataset, train_dataset, val_dataset, test_dataset, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlaYAGn0FY3k",
        "outputId": "3213b488-7bc0-4fac-ab22-d584b72f4e49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Means: tensor([0.1702])\n",
            "STDs: tensor([0.3224])\n"
          ]
        }
      ],
      "source": [
        "dataset, train_dataset, val_dataset, test_dataset, classes = prepare_img_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mXlSdCGDlRb",
        "outputId": "bb6ecf4a-2aa1-475a-e5db-faa123123d47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a drawing of the dog',\n",
              " 'a drawing of the bus',\n",
              " 'a drawing of the tree',\n",
              " 'a drawing of the spider',\n",
              " 'a drawing of the light_bulb',\n",
              " 'a drawing of the pencil',\n",
              " 'a drawing of the bicycle',\n",
              " 'a drawing of the fish',\n",
              " 'a drawing of the ice_cream',\n",
              " 'a drawing of the airplane',\n",
              " 'a drawing of the bird',\n",
              " 'a drawing of the bear',\n",
              " 'a drawing of the moon',\n",
              " 'a drawing of the banana',\n",
              " 'a drawing of the house',\n",
              " 'a drawing of the pig',\n",
              " 'a drawing of the apple',\n",
              " 'a drawing of the hamburger',\n",
              " 'a drawing of the sun',\n",
              " 'a drawing of the shoe',\n",
              " 'a drawing of the rabbit',\n",
              " 'a drawing of the flower',\n",
              " 'a drawing of the monkey',\n",
              " 'a drawing of the cup',\n",
              " 'a drawing of the baseball',\n",
              " 'a drawing of the nose',\n",
              " 'a drawing of the umbrella',\n",
              " 'a drawing of the cat',\n",
              " 'a drawing of the lion',\n",
              " 'a drawing of the duck']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generate sentences\n",
        "clip_labels = [f\"a drawing of the {label}\" for label in classes]\n",
        "clip_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeCo5G5RDZAk"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  zeroshot_weights = []\n",
        "  label_tokens = clip.tokenize(clip_labels).cuda()\n",
        "  label_embs = model.encode_text(label_tokens).float()\n",
        "  label_embs /= label_embs.norm(dim=-1, keepdim=True)\n",
        "  label_emb = label_embs.mean(dim=0)\n",
        "  label_emb /= label_emb.norm()\n",
        "  zeroshot_weights.append(label_emb)\n",
        "  zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thy_3nzpDqvi",
        "outputId": "0d39cf79-e92b-4f53-965d-b00fbf682edf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0023],\n",
              "        [ 0.0023],\n",
              "        [ 0.0099],\n",
              "        ...,\n",
              "        [ 0.0038],\n",
              "        [-0.0046],\n",
              "        [ 0.0364]], device='cuda:0')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zeroshot_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL3PNhsREaKp",
        "outputId": "b0e1f8c0-a6e0-4ddf-da15-1621efa0fd00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x7faa28ab9e50>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuDJqsSwJPn4"
      },
      "outputs": [],
      "source": [
        "def idx_to_class(y_idx):\n",
        "  return classes[int(y_idx.item())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUzFL9DDAuT0",
        "outputId": "17664428-6365-4833-d8ab-1bb2c78f0d6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [14:08<00:00, 35.34it/s]\n"
          ]
        }
      ],
      "source": [
        "img_features = []\n",
        "img_labels = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for img, y in tqdm(test_dataset):\n",
        "  img_input = preprocess(img).unsqueeze(0).cuda()\n",
        "  label = idx_to_class(y)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    img_feature = model.encode_image(img_input)\n",
        "  \n",
        "  img_feature /= img_feature.norm()\n",
        "  img_features.append(img_feature)\n",
        "  img_labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SsuCYNm6Pv_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "53uW8vlEETKz",
        "outputId": "116c51fb-052c-48d9-803b-84c802f8a1ca"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-3e0bb2ec4001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# compute top-1 accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not collections.deque"
          ]
        }
      ],
      "source": [
        "img_features = np.stack(img_features, dim=1).squeeze(0).cuda()\n",
        "\n",
        "    \n",
        "    \n",
        "# compute top-1 accuracy\n",
        "logits = (100. * img_features @ zeroshot_weights).softmax(dim=-1)\n",
        "img_labels = torch.tensor(img_labels).unsqueeze(dim=1).cuda()\n",
        "top1_acc = accuracy(logits, img_labels, (1,))\n",
        "print(f'top-1 accuracy for QuickDraw dataset: {top1_acc[0]:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eae3WDIHKOrP"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "def validate(CONFIG, val_loader, model, criterion, device):\n",
        "    # Reference: https://github.com/pytorch/examples/blob/00ea159a99f5cb3f3301a9bf0baa1a5089c7e217/imagenet/main.py#L313-L353\n",
        "    losses = AverageMeter(\"Loss\", \":.4f\", Summary.AVERAGE)\n",
        "    top1 = AverageMeter(\"Acc@1\", \":6.2f\", Summary.AVERAGE)\n",
        "    top5 = AverageMeter(\"Acc@5\", \":6.2f\", Summary.AVERAGE)\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader), [losses, top1, top5], prefix=\"Validation: \"\n",
        "    )\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, flipped_images, labels) in enumerate(tqdm(val_loader)):\n",
        "            images = images.to(device)\n",
        "            flipped_images = flipped_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # compute logits\n",
        "            logit_original = model(images)\n",
        "            logit_flipped = model(flipped_images)\n",
        "            logit_output = (logit_original + logit_flipped) / 2\n",
        "            \n",
        "            # get cross entropy loss\n",
        "            ce_loss_org = criterion(logit_original, labels)\n",
        "            ce_loss_flip = criterion(logit_flipped, labels)\n",
        "            ce_loss = (ce_loss_org + ce_loss_flip) / 2\n",
        "\n",
        "            # get kl divergence between logits\n",
        "            kl_loss_org = F.kl_div(F.log_softmax(logit_original, dim=-1), F.softmax(logit_flipped, dim=-1), reduction='none')\n",
        "            kl_loss_flip = F.kl_div(F.log_softmax(logit_flipped, dim=-1), F.softmax(logit_original, dim=-1), reduction='none')\n",
        "            kl_loss_org = kl_loss_org.mean()\n",
        "            kl_loss_flip = kl_loss_flip.mean()\n",
        "            kl_loss = (kl_loss_org + kl_loss_flip) / 2\n",
        "\n",
        "            # get crossentropy loss regularized with kl divergence loss\n",
        "            loss = ce_loss + CONFIG.reg_lamda * kl_loss\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(logit_output.data, labels, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        progress.display_summary()\n",
        "    return (\n",
        "        losses.avg,\n",
        "        top1.avg,\n",
        "        top5.avg,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def class_wise(model, model_name, test_dataset, test_loader, device):\n",
        "  name_classes = [k for k, v in test_dataset.class_to_idx.items()]\n",
        "  num_classes = len(name_classes)\n",
        "  confusion_matrix = np.zeros((num_classes, num_classes))\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (X, X_flipped, y) in test_loader:\n",
        "        X = X.type(torch.float32).to(device)\n",
        "        X_flipped = X_flipped.type(torch.float32).to(device)\n",
        "        y = y.type(torch.LongTensor).to(device)\n",
        "        y_pred = model(X).argmax(dim=1, keepdim = True)\n",
        "        y_flipped_pred = model(X_flipped).argmax(dim=1, keepdim = True)\n",
        "        logit_pred = (y_pred + y_flipped_pred) / 2\n",
        "        \n",
        "        for truth, pred, in zip(y.view(-1), logit_pred.view(-1)):\n",
        "          confusion_matrix[truth.long(), pred.long()] += 1\n",
        "  \n",
        "  df_cm = pd.DataFrame(confusion_matrix, index=name_classes, columns=name_classes).astype(int)\n",
        "  \n",
        "  heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='GnBu')\n",
        "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=10)\n",
        "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=10)\n",
        "  plt.ylabel('Truth', fontsize = 12, labelpad=5)\n",
        "  plt.xlabel('Prediction', fontsize = 12, labelpad=5)\n",
        "  plt.title(f'{model_name} Test Accuracy', fontsize = 15, pad=20)\n",
        "  save_figure(f'{model_name}_class_wise_test_accuracy')\n",
        "\n",
        "  test_wise_accuracy_list = np.diag(confusion_matrix) / confusion_matrix.sum(1)\n",
        "  print(test_wise_accuracy_list)\n",
        "\n",
        "  return test_wise_accuracy_list"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
